---
title: "Airline Satisfaction"
author: 
  - Marc Parcerisa
  - Albert Puiggr√≤s
  - Dani Reverter
output:
    pdf_document: default
    html_document: default
    md_document: 
      variant: markdown_github
editor_options:
  chunk_output_type: console
---

<!-- README.md is generated from AirlineSatisfaction.Rmd. Please edit that file. --> 

This project aims to predict airline passenger satisfaction levels by developing
and analyzing binary regression models. Using a dataset containing customer
demographics, flight details, and satisfaction ratings for various services, the
project explores key factors influencing satisfaction. 

This work is organized as follows: [Section 1](#data-preparation) covers data
preparation, including data transformation and cleaning. [Section 2](#eda)
explores each variable in the dataset in detail, whilst [Section 3](#profiling)
searches for relationships between variables and satisfaction levels. 
[Section 4](#model-building), having studied the data, iteratively builds a
logistic regression model to predict satisfaction. [Section 5](#model-validation)
introduces the validation metrics used, and uses them to compare two of the best
models selected on the previous section. Finally, [Section 6](#conclusion)
gives a brief interpretation of the results.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = "images/")
```

```{r, include=FALSE}
# Load libraries
library(FactoMineR)
library(cluster)
library(missMDA)
library(corrplot)
library(pROC)
library(ggplot2)
library(PRROC)
```

# 1. Data preparation {#data-preparation}

Data preparation will be broken down into two main steps. The first step 
involves transforming the data into a usable format, converting factors into 
such and removing useless columns, and will be applied to both training and
testing datasets. The second one will focus on cleaning the data, removing
missing values, and outliers, and will be applied only to the training dataset.

## Data transformation

We'll start by defining a new function that will perform the first step of data
preparation. This function will remove the "X" and "id" columns, and convert
most of the columns to factors.

```{r}
prepareDataset <- function(data) {
  "Prepare dataset for analysis"
  # Remove useless columns "X" and "id"
  data$X <- NULL
  data$id <- NULL

  # Convert columns to factors
  data$Gender <- as.factor(data$Gender)
  data$Customer.Type <- as.factor(data$Customer.Type)
  data$Type.of.Travel <- as.factor(data$Type.of.Travel)
  data$Class <- factor(data$Class, levels = c("Eco", "Eco Plus", "Business"), ordered = TRUE)

  stais_cols <- c(
    "Inflight.wifi.service", "Departure.Arrival.time.convenient",
    "Ease.of.Online.booking", "Gate.location", "Food.and.drink",
    "Online.boarding", "Seat.comfort", "Inflight.entertainment",
    "On.board.service", "Leg.room.service", "Baggage.handling",
    "Checkin.service", "Inflight.service", "Cleanliness"
  )
  for (col in stais_cols) {
    data[[col]] <- factor(data[[col]], levels = 0:5, ordered = TRUE)
  }

  data$satisfaction <- as.factor(data$satisfaction)
  return(data)
}
```

Now we'll load the training and testing datasets and apply the `prepareDataset`
function to both.

```{r}
# Load datasets
train <- read.csv("data/train_data.csv")
test <- read.csv("data/test_data.csv")

# Prepare datasets
train <- prepareDataset(train)
test <- prepareDataset(test)
```

```{r, include=FALSE}
rm(prepareDataset)
gc()
```

Let's take a look at the first few rows of the training dataset to ensure that
the transformation was successful.

```{r}
str(train)
```

## Data cleaning

This next step will focus on cleaning the training dataset. We'll start by 
removing duplicate rows, counting and imputing missing values.

```{r}
# Find duplicate rows
duplicates <- train[duplicated(train), ]
cat("Number of duplicate rows:", nrow(duplicates), "\n")
```

```{r, include=FALSE}
rm(duplicates)
gc()
```

We found no duplicates. Let's now count the missing values in each column.

```{r}
# Count missing values
missing_values <- colSums(is.na(train))
for (col in names(missing_values)) {
  if (missing_values[col] > 0) {
    cat("Column", col, "has", missing_values[col], "missing values\n")
  }
}
```

There's 12 missing values in the "Arrival.Delay.in.Minutes" column. As most of
our columns are factors, we would impute them using Multiple Correspondence
Analysis (MCA). However, the dataset at hand is too large for this method to be
feasible. Instead, seeing as the number of missing values is small, we'll
simply remove the rows with missing values.

```{r}
# Remove rows with missing values
before <- nrow(train)
train <- train[complete.cases(train), ]
after <- nrow(train)
cat("Removed", before - after, "rows with missing values\n")
```

```{r, include=FALSE}
rm(before, after, missing_values)
gc()
```

Now that we've removed the rows with missing values, let's check for outliers in
the numerical columns.

```{r}
# Check for outliers
numeric_df <- Filter(is.numeric, train)
# There's 4 numerical columns
par(mfrow = c(2, 2))
for (col in names(numeric_df)) {
  boxplot(numeric_df[[col]], main = col, horizontal = TRUE)

  # Calculate IQR
  q1 <- quantile(numeric_df[[col]], 0.25)
  q3 <- quantile(numeric_df[[col]], 0.75)
  iqr <- q3 - q1

  # Add the lines to the plot
  abline(v = c(q1 - 1.5 * iqr, q3 + 1.5 * iqr), col = "orange")
  abline(v = c(q1 - 3 * iqr, q3 + 3 * iqr), col = "red")

  # Find outliers
  mild_outliers <- which(
    numeric_df[[col]] < (q1 - 1.5 * iqr) & numeric_df[[col]] > (q1 - 3 * iqr) |
      numeric_df[[col]] > (q3 + 1.5 * iqr) & numeric_df[[col]] < (q3 + 3 * iqr)
  )
  severe_outliers <- which(numeric_df[[col]] < (q1 - 3 * iqr) | numeric_df[[col]] > (q3 + 3 * iqr))


  cat("Column", col, "has", length(mild_outliers), "mild outliers and", length(severe_outliers), "severe outliers\n")
}
```

```{r, include=FALSE}
rm(numeric_df, q1, q3, iqr, mild_outliers, severe_outliers, col)
gc()
```

Looking at the boxplots, we can see that there are no severe outliers in neither
"Age" nor "Flight.Distance", but there are some in "Departure.Delay.in.Minutes"
and "Arrival.Delay.in.Minutes". However, none of them look too extreme to be
considered errors. Moreover, knowing as we aim to predict satisfaction, arguably
the more extreme the delays are, the more likely the passenger is to be
dissatisfied, providing valuable modeling information. Therefore, we'll keep
them as they are.

Instead, knowing that the arrival and departure delays must be strongly 
correlated, we'll plot them to check if any anomalies are present, indicating
errors in the data.

```{r}
# Plot departure vs arrival delays
par(mfrow = c(1, 1))
plot(train$Departure.Delay.in.Minutes, train$Arrival.Delay.in.Minutes, xlab = "Departure Delay (min)", ylab = "Arrival Delay (min)")
abline(a = 0, b = 1, col = "red")
```

No points seem to be too far from the red line, so we'll consider them as valid
data points.

Before proceeding, we must note that there is a slight imbalance in the target 
variable "Satisfaction", which could affect the model's performance. We'll
address this issue in later sections.

```{r}
table(train$satisfaction)
colnames(train)
```

# 2. Exploratory Data Analysis {#eda}

## Variable 1: Gender
This is a Nominal variable with 2 levels (binary). It is visualized by a bar
plot, in which it is clear that all levels are well represented. 

```{r}
summary(train$Gender)
plot(train$Gender)
```

## Variable 2: Customer.Type
This is a Nominal variable with 2 levels (binary). It is visualized by a bar
plot, which shows an imbalance between the two groups, with the loyal customers
group being 4x larger than the disloyal customer group. Since the imbalance is
not large enough to affect the model, no action is taken.

```{r}
summary(train$Customer.Type)
plot(train$Customer.Type)
```

## Variable 3: Age
This is a Numeric interval variable. It is visualized by a histogram and a
boxplot to examine its distribution, which appears to be well represented for
people of all ages, with no outliers. Visually, a slight deviation from a normal
distribution (centered at age 40) is seen at the ages 20-30, being slightly
higher represented. 

```{r}
summary(train$Age)
hist(train$Age, breaks = 30)
boxplot(train$Age)
```

## Variable 4: Type.of.Travel
This is a Nominal variable with 2 levels (binary). It is visualized by a bar
plot, in which it is clear that all levels are well represented, with Business
travel having 2x the individuals than Personal travel.

```{r}
summary(train$Type.of.Travel)
plot(train$Type.of.Travel)
```

## Variable 5: Class
This is a Nominal variable with 3 levels. It is visualized by a bar plot, in
which one of the levels is underrepresented with only 7.23% of the observations.
Since this is still a significant portion of the observations, no action is taken.

```{r}
summary(train$Class)
plot(train$Class)
```

## Variable 6: Flight.Distance
This is a continuous ratio variable. It is visualized by a histogram and a
boxplot to examine its distribution. A few univariate outliers are detected, but
they are not so far from the interquartile range to consider changing or
deleting them. It is clearly not normally distributed, confirmed by the
near-null p-value of the shapiro normallity test.

```{r}
summary(train$Flight.Distance)
hist(train$Flight.Distance, breaks = 30)
boxplot(train$Flight.Distance)
shapiro.test(train$Flight.Distance)
```

## Variable 7: Inflight.wifi.service
This is the first of many Nominal variables with 6 levels (0 to 5, customer
satisfaction level). It is visualized by a bar plot, in which it is clear that
all levels are well represented.

```{r}
summary(train$Inflight.wifi.service)
plot(train$Inflight.wifi.service)
```

##Variable 8: Departure.Arrival.time.convenient
This is another Nominal variable with 6 levels. It is visualized by a bar plot,
in which it is clear that all levels are well represented.

```{r}
summary(train$Departure.Arrival.time.convenient)
plot(train$Departure.Arrival.time.convenient)
```

## Variable 9: Ease.of.Online.booking
This is another Nominal variable with 6 levels. It is visualized by a bar plot,
in which it is clear that all levels are well represented.

```{r}
summary(train$Ease.of.Online.booking)
plot(train$Ease.of.Online.booking)
```

## Variable 10: Gate.location
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
One of the levels is not represented at all, but is not deleted in case the test
dataset contains observations at this level.

```{r}
summary(train$Gate.location)
plot(train$Gate.location)
```

## Variable 11: Food.and.drink
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
The 0 level is very underrepresented, but as before is not deleted in case it is
not underrepresented in the test dataset.

```{r}
summary(train$Food.and.drink)
plot(train$Food.and.drink)
```

## Variable 12: Online.boarding
This is another Nominal variable with 6 levels. It is visualized by a bar plot,
in which it is clear that all levels are well represented.

```{r}
summary(train$Online.boarding)
plot(train$Online.boarding)
```

## Variable 13: Seat.comfort
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
The 0 level is not represented, but as before is not deleted in case it is not
underrepresented in the test dataset.

```{r}
summary(train$Seat.comfort)
plot(train$Seat.comfort)
```

## Variable 14: Inflight.entertainment
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
The 0 level is very underrepresented, but as before is not deleted in case it is
not underrepresented in the test dataset.

```{r}
summary(train$Inflight.entertainment)
plot(train$Inflight.entertainment)
```

## Variable 15: On.board.service
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
The 0 level is very underrepresented, but as before is not deleted in case it is
not underrepresented in the test dataset.

```{r}
summary(train$On.board.service)
plot(train$On.board.service)
```

## Variable 16: Leg.room.service
This is another Nominal variable with 6 levels. It is visualized by a bar plot,
in which it is clear that all levels are well represented.

```{r}
summary(train$Leg.room.service)
plot(train$Leg.room.service)
```

## Variable 17: Baggage.handling
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
The 0 level is not represented, but as before is not deleted in case it is
represented in the test dataset. 

```{r}
summary(train$Baggage.handling)
plot(train$Baggage.handling)
```

## Variable 18: Checkin.service
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
The 0 level is not represented, but as before is not deleted in case it is
represented in the test dataset. 

```{r}
summary(train$Checkin.service)
plot(train$Checkin.service)
```

## Variable 19: Inflight.service
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
The 0 level is very underrepresented, but as before is not deleted in case it is
not underrepresented in the test dataset.

```{r}
summary(train$Inflight.service)
plot(train$Inflight.service)
```

## Variable 20: Cleanliness
This is another Nominal variable with 6 levels. It is visualized by a bar plot.
The 0 level is not represented, but as before is not deleted in case it is
represented in the test dataset. 

```{r}
summary(train$Cleanliness)
plot(train$Cleanliness)
```

## Variable 21: Departure.Delay.in.Minutes
This is a continuous ratio variable. It is visualized by a histogram and a
boxplot to examine its distribution. 56.70% of the observations have a delay of
0 minutes, and the variable clearly doesn't follow a normal distribution,
confirmed by a shapiro test. Due to its nature, it contains a lot of outliers
(almost any observation that is not 0) so no action is taken to change/delete
them since they represent a large percentage of the observations.

```{r}
summary(train$Departure.Delay.in.Minutes)
mean(train$Departure.Delay.in.Minutes == 0) * 100
hist(train$Departure.Delay.in.Minutes, breaks = 30)
boxplot(train$Departure.Delay.in.Minutes)
shapiro.test(train$Departure.Delay.in.Minutes)
```

## Variable 22: Arrival.Delay.in.Minutes
This is a continuous ratio variable. It is visualized by a histogram and a
boxplot to examine its distribution. Similarly to the last variable, 55.77%  of
the observations have a delay of 0 minutes, and the variable clearly doesn't
follow a normal distribution, confirmed by a shapiro test. Due to its nature, it
contains a lot of outliers (almost any observation that is not 0) so no action
is taken to change/delete them since they represent a large percentage of the
observations.

```{r}
summary(train$Arrival.Delay.in.Minutes)
mean(train$Arrival.Delay.in.Minutes == 0) * 100
hist(train$Arrival.Delay.in.Minutes, breaks = 30)
boxplot(train$Arrival.Delay.in.Minutes)
shapiro.test(train$Arrival.Delay.in.Minutes)
```

## Variable 23: Satisfaction
This is our response variable. It is a Nominal variable with 2 levels (binary).
It is visualized by a bar plot, in which it is clear that all levels are well
represented.

```{r}
summary(train$satisfaction)
plot(train$satisfaction)
```

# 3. Profiling {#profiling}

To analyze the categorical response variable satisfaction, the catdes function
from the FactoMineR package was used. This function identifies the relationship
between the target variable and the other quantitative and qualitative variables.

The chi-squared test results show that several qualitative variables are
significantly associated with the satisfaction levels, as indicated by their
extremely low p-values. The variables most strongly associated with satisfaction
(in order of decreasing significance) are Online.boarding, Inflight.wifi.service,
Class, Type.of.Travel and Inflight.entertainment. 

For the quantitative variables, the Eta¬≤ values and p-values provide insight
into their relationship with satisfaction. The most influential variables
include Flight.Distance (passengers with longer flight distances tend to report
higher satisfaction, as indicated by the positive mean in the satisfied category),
Age (older passengers are more likely to report satisfaction, with a higher mean
age in the satisfied category compared to neutral or dissatisfied),
Arrival.Delay.in.Minutes and Departure.Delay.in.Minutes (both delays are
negatively associated with satisfaction, as expected, though their impact is
relatively small)

Passengers categorized as satisfied have a significantly higher mean in
Flight.Distance and Age compared to the overall averages.

Passengers categorized as neutral or dissatisfied have lower mean
Flight.Distance and Age. Both Departure.Delay.in.Minutes and
Arrival.Delay.in.Minutes are slightly higher.

The analysis reveals that satisfaction is most strongly influenced by
qualitative variables such as Online.boarding, Inflight.wifi.service, and Class.
Among quantitative variables, Flight.Distance and Age are the most significant
predictors.

```{r}
res.cat <- catdes(train, 23)
res.cat$test.chi2
res.cat$quanti.var
res.cat$quanti
```

# 4. Feature Selection & Model Building {#model-building}

```{r}
numeric_vars <- sapply(train, is.numeric)
summary(train[, numeric_vars])
corr_matrix <- cor(train[, numeric_vars], use = "complete.obs")
corrplot(corr_matrix)

train1 <- train
# CHI-TESTS(Testing independence between two Factors)
variables_to_clean <- c(
  "Gate.location", "Food.and.drink",
  "Inflight.entertainment", "Seat.comfort",
  "On.board.service", "Baggage.handling",
  "Checkin.service", "Inflight.service", "Cleanliness"
)

for (var in variables_to_clean) {
  train1 <- train1[train1[[var]] != "0", ]
  train1[[var]] <- factor(train1[[var]])
}

categorical_vars <- c(
  "Gender",
  "Customer.Type", "Type.of.Travel", "Class",
  "Inflight.wifi.service", "Departure.Arrival.time.convenient",
  "Cleanliness", "Food.and.drink", "Online.boarding",
  "Seat.comfort", "Inflight.entertainment", "On.board.service",
  "Leg.room.service", "Baggage.handling", "Checkin.service"
)

for (var in categorical_vars) {
  contingency_table <- table(train1[[var]], train1$satisfaction)
  chi_test <- chisq.test(contingency_table)
  cat("\nChi-Square Test for", var, "vs Satisfaction:\n")
  print(chi_test)
}
```

Regarding numerical variables, the correlation matrix shows that Age and Flight.
Distance have a low positive correlation (0.1147), indicating minimal 
association between the two variables. The Arrival.Delay.in.Minutes and
Departure.Delay.in.Minutes are highly correlated (0.9676), suggesting a strong
linear relationship between them.

The Chi-Square tests show that categorical variables such as Customer.Type,
Type.of.Travel, Class, Inflight.wifi.service, Cleanliness, Seat.comfort,
Inflight.entertainment, On.board.service, Leg.room.service, Baggage.handling,
and Checkin.service all have highly significant p-values (all < 2.2e-16),
indicating a strong relationship with satisfaction. In contrast, Gender and
Departure.Arrival.time.convenient have less impact, with p-values of 0.6884 and
0.01553, respectively. These results suggest that the most important factors for
modeling satisfaction are related to customer type, travel type, flight class,
and the quality of services like inflight entertainment, wifi, cleanliness, and
comfort during the flight. To perform the Chi-Square test, we had to eliminate
the categories of the categorical variables that were empty.

## Numeric Variables

```{r}
m0 <- glm(satisfaction ~ 1, data = train, family = binomial)

summary(m0)

m1 <- glm(satisfaction ~ Flight.Distance, data = train, family = binomial)

summary(m1)
anova(m0, m1)

m1 <- glm(satisfaction ~ Flight.Distance, data = train, family = binomial)

summary(m1)
anova(m0, m1)

m2 <- glm(satisfaction ~ Flight.Distance + Age, data = train, family = binomial)

summary(m2)
anova(m1, m2)

m3 <- glm(satisfaction ~ Flight.Distance + poly(Age, 2), data = train, family = binomial)

summary(m3)
anova(m2, m3)


m4 <- glm(satisfaction ~ Flight.Distance + poly(Age, 2), data = train, family = binomial)

summary(m4)
anova(m3, m4)


m5 <- glm(satisfaction ~ Flight.Distance + poly(Age, 2) + Arrival.Delay.in.Minutes, data = train, family = binomial)

summary(m5)
anova(m4, m5)
Anova(m5)

marginalModelPlots(m5)
residualPlots(m5)
```

To assess the logistic regression model for the target variable "satisfaction,"
we began with the null model, which included only the intercept, to evaluate the
improvement achieved by adding new predictors. The null deviance of the model
was 6812.3, with an AIC of 6814.3. The first predictor added was
`Flight.Distance`, which significantly improved the model fit, reducing the
deviance to 6355.9 and the AIC to 6359.9. The second numerical variable included
was `Age`, which also led to a notable improvement. To capture non-linear
effects of `Age`, we refined the model by introducing a second-order polynomial
term for `Age`. Finally, we incorporated `Arrival.Delay.in.Minutes`, which
further improved the model, reducing the deviance to 6042.3 and the AIC to 6052.3.

To evaluate the improvement of the models, we used the ANOVA test. The results
indicated that each addition of a predictor significantly improved the model, as
evidenced by the reductions in deviance and corresponding p-values. In summary,
the most critical improvements werethe addition of `Flight.Distance`, the
refinement of `Age` using polynomial terms, and the inclusion of
`Arrival.Delay.in.Minutes`, all of which were confirmed by the ANOVA tests.

## Factors & Interaction Terms

```{r}
par(mfrow = c(1, 1))
# STEP
initial_model <- glm(
  satisfaction ~ Gender + Customer.Type + poly(Age, 2) + Arrival.Delay.in.Minutes + (Type.of.Travel +
    Class + Flight.Distance + Inflight.wifi.service + Departure.Arrival.time.convenient +
    Ease.of.Online.booking + Gate.location + Food.and.drink + Online.boarding +
    Seat.comfort + Inflight.entertainment + On.board.service + Leg.room.service +
    Baggage.handling + Checkin.service + Inflight.service + Cleanliness +
    Departure.Delay.in.Minutes)^2,
  data = train, family = binomial
)

stepwise_model <- step(initial_model, direction = "both", trace = 0)

# RESIDUAL ANALYSIS OF THE STEP MODEL
marginalModelPlots(stepwise_model)
residualPlots(stepwise_model)

coef(stepwise_model)
Anova(stepwise_model, test = "LR")
AIC(stepwise_model)
vif(stepwise_model)
vif(stepwise_model, type = "predictor")
```

# 5. Model Validation {#model-validation}

Using the training dataset, we have built a logistic regression model to predict
passenger satisfaction. Now, we will evaluate the model's performance using the
testing dataset.

Let's start by predicting the satisfaction levels for the testing dataset.

```{r}
test$Satisfaction.Prob <- predict(stepwise_model, newdata = test, type = "response")
```

Due to the `type="response"` parameter, the new column "Satisfaction.Prob"
contains the predicted probabilities of satisfaction for each passenger. To
decide whether a passenger is satisfied or not, a threshold must be set. For
example, let's set a threshold of 0.5.

```{r}
test$Predicted.Satisfaction <- ifelse(test$Satisfaction.Prob > 0.5, "satisfied", "neutral or dissatisfied")
```

And with that, a confusion matrix can be created to evaluate the model's
performance.

```{r}
conf_matrix <- table(test$satisfaction, test$Predicted.Satisfaction)
```

To calculate the performance metrics, we start from the confusion matrix's elements:

```{r}
TP <- conf_matrix[2, 2]
TN <- conf_matrix[1, 1]
FP <- conf_matrix[1, 2]
FN <- conf_matrix[2, 1]
```

We define "Accuracy" as the Correct Classification Rate:

$$
CCR = \frac{TP + TN}{TP + TN + FP + FN}
$$

```{r}
(TP + TN) / sum(conf_matrix)
```

We define "Sensitivity" or "Recall" as the True Positive Rate:

$$
TPR = \frac{TP}{TP + FN}
$$

```{r}
TP / (TP + FN)
```

And we define "Fall-out" as the False Positive Rate:

$$
FPR = \frac{FP}{FP + TN}
$$

```{r}
FP / (FP + TN)
```

The process of designing a model and choosing a threshold consists of a careful
trade-off between sensitivity and fall-out. If we plot these metrics against
different thresholds, we can visualize this trade-off in what is known as a
Receiver Operating Characteristic (ROC) curve:

```{r}
par(mfrow = c(1, 1))
roc <- roc(test$satisfaction, test$Predicted.Satisfaction)
# Set limits to 0..1
plot(roc, main = "ROC Curve", col = "blue")
```

A model with perfect discrimination has an ROC curve that passes through the
upper-left corner, whilst a model with no discrimination has an ROC curve that
passes through the diagonal. Thus, the area under the ROC curve (AUC) is a
good measure of the model's performance. The closer the AUC is to 1, the better
the model is at distinguishing between satisfied and dissatisfied passengers.

```{r}
auc(roc)
```

This high AUC value indicates that the model is fairly good at distinguishing
between satisfied and dissatisfied passengers.

## Comparison

```{r}
# Final model
m6 <- glm(
  satisfaction ~ Flight.Distance + poly(Age, 2) + Arrival.Delay.in.Minutes + Customer.Type + Type.of.Travel +
    Class + Inflight.wifi.service + Cleanliness + Food.and.drink + Online.boarding + Seat.comfort +
    Inflight.entertainment + On.board.service + Leg.room.service + Baggage.handling + Checkin.service,
  data = train, family = binomial
)

summary(m6)

marginalModelPlots(m6)
residualPlots(m6)


prob_pred1 <- predict(stepwise_model, newdata = test, type = "response")
prob_pred2 <- predict(m6, newdata = test, type = "response")

roc_model1 <- roc(test$satisfaction_binary, prob_pred1, quiet = TRUE)
roc_model2 <- roc(test$satisfaction_binary, prob_pred2, quiet = TRUE)

plot(roc_model1, col = "blue", lwd = 2, main = "ROC Curve Comparison", legacy.axes = TRUE)
polygon(c(roc_model1$specificities, 0), c(roc_model1$sensitivities, 0), col = adjustcolor("blue", alpha.f = 0.2), border = NA)


lines(roc_model2, col = "red", lwd = 2)
polygon(c(roc_model2$specificities, 0), c(roc_model2$sensitivities, 0), col = adjustcolor("red", alpha.f = 0.2), border = NA)


legend("bottomright",
  legend = c("Model 1", "Model 2"),
  col = c("blue", "red"), lwd = 2, fill = adjustcolor(c("blue", "red"), alpha.f = 0.2)
)

auc(roc_model1)
auc(roc_model2)

###
valid_indices <- !is.na(prob_pred)
prob_pred <- prob_pred[valid_indices]
test <- test[valid_indices, ] # Subset the test dataset

test$satisfaction_binary <- ifelse(test$satisfaction == "satisfied", 1, 0)

pr_curve <- pr.curve(scores.class0 = prob_pred, weights.class0 = test$satisfaction_binary, curve = TRUE)
plot(pr_curve, main = "Precision-Recall Curve", col = "red", lwd = 1)
```

# 6. Conclusion {#conclusion}

<!-- Interpret the results :) -->